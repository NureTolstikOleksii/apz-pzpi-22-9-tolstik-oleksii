Міністерство освіти і науки України
Харківський національний університет радіоелектроніки


Кафедра програмної інженерії



Звіт
з лабораторної роботи №4
з дисципліни "Архітектура програмного забезпечення"
з теми: "Масштабування бекенда"





Виконав 								Перевірив
ст. гр. ПЗПІ-22-9 							Дашенков Д. С.
Толстік О. В.









2025
1 ІСТОРІЯ ЗМІН

Таблиця 1 – Історія змін
№	Дата	Версія звіту	Опис змін та виправлень
1	22.05.2025	0.1	Створено звіт

2 ЗАВДАННЯ

Тема: Масштабування бекенду.
В цій лабораторній роботі необхідно показати як можна масштабувати бекенд системи для роботи із великим навантаженням. Для цього, можна на вибір: масштабувати сервер горизонтально – багато копій сервера виконують однакові функції для різних користувачів; масштабувати сервер вертикально – різні мікросервіси виконують різні функції і масштабуються окремо одне від одного. На найвищий бал на цю роботу необхідно провести навантажувальне тестування за допомогою Gatling, JMeter, Locust чи іншого подібного інструмента і показати як зі збільшенням кількості серверів зростає кількість запитів на секунду яку витримує система.

3 ОПИС ВИКОНАНОЇ РОБОТИ

3.1 Опис стратегії масштабування системи

У межах масштабування бекенду було реалізовано масштабування серверної частини системи HealthyHelper у середовищі Kubernetes. Для цього було створено Deployment, у якому задається кількість реплік (pod-ів), що відповідають за обробку запитів.
У якості стратегії масштабування використано горизонтальне масштабування, при якому кількість ідентичних pod-ів змінюється вручну.
Також було застосовано об'єкт типу Service з параметром type: LoadBalancer (рис 3.1), який забезпечує балансування навантаження між кількома подами. Таким чином, усі зовнішні запити, які надходять на систему, автоматично розподіляються між активними екземплярами бекенду.

 
Рисунок 3.1 – Застосування об'єкт типу Service з параметром type: LoadBalancer

3.2 Опис технічних рішень, які роблять масштабування можливим

Масштабування у реалізованій системі стало можливим завдяки модульній архітектурі бекенду, побудованій на основі Express.js, яка не зберігає жодного стану на рівні сервера. Усі запити до API незалежні, а всі дані зберігаються централізовано в зовнішній базі даних PostgreSQL, що дозволяє створювати скільки завгодно реплік серверного застосунку без втрати цілісності чи конфліктів стану.
Архітектура була адаптована для розгортання в Kubernetes, де основними технічними рішеннями, що забезпечили масштабування, стали:
- deployment-ресурс, який задає шаблон створення pod-ів сервера й дозволяє гнучко змінювати їхню кількість за потреби;
- service типу LoadBalancer, який розподіляє вхідні HTTP-запити між усіма активними pod-ами застосунку автоматично, що дозволяє ефективно використовувати ресурси та уникати перевантаження окремих інстансів;
- окремий pod з PostgreSQL, який має персистентне сховище (PVC), і є спільним джерелом даних для всіх екземплярів сервера.
Завдяки цим архітектурним і технічним рішенням система підтримує горизонтальне масштабування – можливість додавати нові pod-и бекенду без змін у коді та з автоматичним балансуванням навантаження.

3.3 Опис навантажувальних тестів

Для тестування системи було використано інструмент Locust, який дозволяє змоделювати паралельне навантаження з великої кількості користувачів. Тестування проводилось на маршруті /patients, що є типовим для системи і виконує запит до бази даних з поверненням списку пацієнтів.
З метою оцінки масштабованості, було проведено три окремі сесії тестування з різною кількістю подів (серверів), які обробляють запити:1 pod, 2 pod-и, та 3 pod-и. Для кожного сценарію симулювалось 1000 одночасних користувачів зі швидкістю появи 25 користувачів/секунда. Під час кожного тесту фіксувались наступні показники:
- кількість оброблених запитів на секунду (RPS);
- середній час відповіді (latency);
- максимальна затримка;
- кількість помилок (failures).
Тестування тривало в середньому 3 хвилини для кожної конфігурації.
У процесі деплою та тестування системи було створено та використано наступні файли YAML та конфігурації:
- deployment.yaml – описує бекенд-деплоймент системи (сервер на Express.js), містить конфігурацію для створення одного або кількох pod-ів з Docker-образом сервера;
- service.yaml – створює Kubernetes Service типу LoadBalancer, який забезпечує зовнішній доступ до серверу та балансування навантаження між подами;
- postgres-deployment.yaml – визначає  окремий pod для PostgreSQL бази даних, включає конфігурацію змінних оточення (назва БД, логін, пароль);
- postgres-service.yaml – встановлює внутрішній ClusterIP сервіс для підключення сервера до бази PostgreSQL в середині кластера;
- postgres-pvc.yaml – описує персистентне сховище (PVC) обсягом 1 ГБ для збереження даних PostgreSQL. Завдяки цьому база даних не очищується при перезапуску pod-ів.
Для тестування створено:
- locustfile.py – python-скрипт для навантажувального тестування. Він містить код, який автоматично виконує HTTP-запити до /patients і вимірює продуктивність системи.

3.4 Аналіз результатів навантаження

Як було сказано вище, у процесі тестування було проведено три сесії навантажувального моделювання з використанням інструмента Locust при однаковому навантаженні – 1000 одночасних користувачів. Для кожної сесії змінювалась кількість pod-ів, які обслуговують сервер: 1, 2 та 3 відповідно.
1 pod:
При тестуванні системи на одному pod-і було досягнуто стабільного обслуговування без помилок. Середній час відповіді становив 803 мс, а піковий – понад 5.6 секунд. Система стабільно обробляла близько 366 запитів на секунду. Проте з графіків видно, що латентність поступово зростала, що свідчить про необхідність масштабування при підвищеному навантаженні.

 
Рисунок 3.2 – Результат тестування системи на 1-му поді

 
Рисунок 3.3 – Результат тестування системи на 1-му поді

 
Рисунок 3.4 – Результат тестування системи на 1-му поді
2 pods:
Далі виконуємо наступну команду для збільшення кількості подів:
kubectl scale deployment healthyhelper-deployment --replicas=2
При масштабуванні до двох pod-ів система демонструє стабільне обслуговування навантаження 1000 користувачів. Середній час відповіді залишився на рівні 808 мс, але максимальні затримки знизились. Система стабільно обробляла 370 запитів на секунду без помилок. Зростання кількості pod-ів покращило здатність системи витримувати тиск, хоча RPS незначно зросла, ймовірно через обмеження по базі даних.

 
Рисунок 3.5 – Результат тестування системи на 2-ох подах
 
Рисунок 3.6 – Результат тестування системи на 2-ох подах

3 pods:
Виконуємо команду для збільшення подів до 3:
kubectl scale deployment healthyhelper-deployment --replicas=3
У конфігурації з трьома pod-ами система продемонструвала найкращі результати: максимальний RPS сягнув 382.25 запитів/сек, середній час відповіді знизився до 709 мс, при цьому жодної помилки зафіксовано не було. Графіки підтверджують стабільність навантаження, рівномірний розподіл трафіку та зменшення латентності порівняно з 1 і 2 pod-ами. Це свідчить про ефективну роботу Kubernetes LoadBalancer і здатність системи до горизонтального масштабування.

 
Рисунок 3.7 – Результат тестування системи на 3-ох подах

 
Рисунок 3.8 – Результат тестування системи на 3-ох подах

Занесемо результати тестування до порівняльної таблиці 1.







Таблиця 1 – Результати тестування системи під навантаженням
Кількість подів	RPS	Середній час відповіді	Максимальний час відповіді	Помилки	Висновки
1	366.13	802 мс	5666 мс	0	високий тиск, зростає latency

2	368.73	800 мс	6231 мс	0	latency стабільна, RPS трохи зростає

3	382.25	709 м	5741 мс	0	latency знижується, RPS вищий


Таким чином, проведене навантажувальне тестування показало, що зі збільшенням кількості pod-ів система демонструє помірне зростання продуктивності. RPS (кількість оброблених запитів на секунду) зросла з 366 до 382 при переході від одного pod-а до трьох, що підтверджує здатність системи масштабуватись. Разом із тим, динаміка зростання продуктивності є слабкою, що свідчить про наявність інших вузьких місць у системі, зокрема обмежень на рівні бази даних або інфраструктури мого комп’ютера.

3.5 Аналіз вузьких місць

Під час проведення навантажувальних тестів із поступовим масштабуванням системи від 1 до 3 pod-ів було виявлено, що при зростанні кількості серверних екземплярів (pod-ів) приріст продуктивності, зокрема RPS (requests per second), був незначним. Це свідчить про наявність вузького місця, яке обмежує загальну ефективність масштабування.
Основним обмежувальним фактором у даній архітектурі виступає централізована база даних PostgreSQL, яка реалізована у вигляді окремого pod-а. Незалежно від кількості pod-ів, саме база обробляє всі запити до даних. У момент навантаження виникає велика кількість одночасних підключень до одного джерела, що призводить до підвищення затримок на рівні СУБД. Оскільки база даних не масштабується разом із бекендом, вона стає першою точкою насичення при підвищенні трафіку.
Крім того, тестування проводилося у середовищі Docker Desktop під Windows, що має обмеження щодо виділених ресурсів (CPU, RAM) для віртуального кластера Kubernetes. Це також могло впливати на продуктивність, особливо при одночасному запуску кількох pod-ів і бази даних в межах одного хосту. 
У реальному хмарному середовищі (наприклад, GKE, EKS, AKS) ці обмеження могли б бути зменшені або усунені завдяки горизонтальному масштабуванню самої бази або використанню managed-сервісів.
Таким чином, першим ресурсом, що вичерпується при зростанні навантаження, є обчислювальна потужність бази даних, а в рамках поточного середовища, – ресурси хост-машини, на якій розгорнутий кластер.

4 ВИСНОВКИ

У ході виконання лабораторної роботи було розгорнуто серверну частину системи в Kubernetes, реалізовано горизонтальне масштабування за допомогою Deployment і LoadBalancer, а також проведено навантажувальні тести з 1, 2 та 3 pod-ами. Тестування показало зростання RPS і зниження затримок при збільшенні кількості pod-ів, що підтверджує ефективність масштабування. Незначне зростання продуктивності вказує на наявність вузького місця на рівні бази даних, проте система працювала стабільно та без помилок навіть за умов високого навантаження.
Посилання на GitHub: https://github.com/NureTolstikOleksii/apz-pzpi-22-9-tolstik-oleksii/tree/main/Lab4
